People often say you don't need to know any math to program a computer, and that's truthy. However, any sufficiently advanced technology is indistinguishable from magic. But magic isn't real. Math explains it all. Developers often avoid learning math because it looks scary, but it actually makes complicated magic like computer graphics and neural networks easier to understand. And not just that, but it unlocks the secrets of the entire physical universe. There's a few abstract math concepts for program grammars that you just can't ignore, and understanding them will make you think more like a true engineer. I'm no math genius. In fact, I'm the opposite, which makes me uniquely qualified to teach ten essential math concepts using memes and other visual AIDS that almost anybody can understand. The first person who survives until the end and cracks the code will win this one of a kind. Balenciaga fireship hoodie. Let's jump right into topic number one boolean algebra, which is used almost every time you write code. A boolean is a binary variable that can have two possible values true or false. Now, there are three operators that you can use to work with them and or and not or, conjunction, disjunction, and negation. If you want to be fancy, imagine you're trying to get a girlfriend. We can use the boolean variables of rich and handsome to describe you. If you're both rich and handsome, you'll definitely get a girlfriend. But if you're not rich and not handsome, you'll have to pay for an OnlyFans girlfriend. If you're rich or handsome, you can still get a girlfriend, but your selection is more limited. We can represent this logic and code with things like if statements, but we can also do it visually with Venn diagrams like you've been doing since grade school. And you can also create truth tables, like this one here, which comes in handy in zombie apocalypse type situations. But that brings us to concept two numeral systems like base two. Throughout history, humans have come up with all kinds of clever ways to count things, but almost all of them use base ten, like the ten fingers on your hands, like in the number 423. The four is in the hundreds place, which represents four times 100. The two is in the tens place, or two times ten. Then the final number is multiplied by one. But computers work off of a base two numeral system. In base two, things work the exact same way. But because we only have two symbols to work with, each place is multiplied by two, like two 4816, 326-4128, and so on. Now that you understand that, you also understand how other bases work, like base 16, or hexadecimal, which uses the digits zero through nine along with A through F, and is commonly used to represent binary values in a more concise way, because each hexadecimal digit can translate into four bits. And you also have base 64, which introduces even more letters and symbols to encode binary values, allowing developers to represent binary data like an image as a string of text. While computers use base two under the hood, they still need to represent base ten numbers for humans. And the most common way to do that is with floating point numbers. Go ahead and execute 0.1 plus zero two in your favorite programming language. The result should be 03004. How could a computer make such a silly mistake? Well, in science, numbers can get very big when talking about space and very small when talking about things like atoms. We don't write these numbers out completely and instead use scientific notation where an exponent raises it to an approximate value, making the number appear far more concise. Computers use a similar approach with floating point numbers because they only have a limited amount of space either 32 bits for single precision or 64 for double precision, which is the default in languages like Python and JavaScript. It's called a floating point because there's no fixed number of digits before or after the decimal point, allowing developers to make a tradeoff between range and precision. Some numbers, like zero one, can't be represented perfectly as a binary floating point, which is the reason we get those tiny rounding errors. Now that you know how numbers work, we can start talking about logarithmic functions, which you might remember from algebra class. To understand them, think of an actual log. The log starts at 16ft, then we saw it in half, over and over again until we reach a length of two. Notice how when we try to draw a line through it, it's not perfectly straight, but rather it curves gradually. In computer science, this is how many algorithms work like binary search. What's interesting, though, is that if we know how many times we cut the log, we can raise two to the power of that value, in this case, four, to get the original length of the log. That's called exponentiation, and logarithm is the exact opposite. Imagine we don't know the exponent, we only know the original length, and want to figure out how many times we'd have to cut the log to get a length of two. We could calculate that with a base two log on the original length, returning a value of four. When it's base two, it's called a binary logarithm. However, in math it's more common to use base ten, which is called the common logarithm. That'll come in handy. But now let's shift gears to set theory. A set is just an unordered collection of unique values. The best example is a relational database, where each table is a set of unique rows. These tables can be joined together in a variety of different ways based on set theory. Like to select records that match on both tables, you do an inner join, which is called an intersection in set theory. Or to grab all matching records from both tables, you do a full outer join. Which is a union in set theory. When doing left and right joins, these concepts are combined, like by combining an intersection with the difference between the two sets. Now, one thing that's important to notice at this point is that we're talking about structures that have a finite number of elements, which falls under the umbrella of discrete math, as opposed to continuous math like geometry and calculus that deal with real numbers and approximations. And that brings us to our next discrete topic combinatorics. Simply put, this is all about counting things, especially when combining sets into combinations or permutations. Like if you have an app like Tinder, you might need to count all the possible combinations of matches as part of a more complex algorithm that figures out what to show the end user. Or maybe you're building a globally distributed database and need to figure out how many database partitions you'll need around the world. Ultimately, it's all about understanding how patterns can emerge. What you're looking at here is the formula for the Fibonacci sequence. A great exercise is to write a function that generates it. Congrats, bro, you just did combinatorics and unlock the secrets of the universe. Engineers at Google use the same kind of logic to render tiles on tools like Google Maps, and the gods use this pattern all over the place. In nature, combinatorics is closely related to graph theory. A graph has two basic parts nodes or vertices and edges that connect them together. Like a person might be a node and an edge might be a relationship that connects them together. Like you love your mom and your mom loves you back. This is an undirected graph because the relationship goes both ways. However, you might also love your OnlyFans girlfriend, but this would be a directed graph because that relationship only goes one way. Edges can also be weighted, meaning one relationship is more important than the other. If a node can traverse back to itself, it's called cyclic. Otherwise it's known as acyclic. As a programmer, you'll often need to build graphs from the ground up, but more importantly, know how to traverse them. Like when using Dijkstra's algorithm to find the most efficient way to navigate through traffic. But before you start writing graph traversal algorithms, you should know a little bit about complexity theory, which can tell us in theory how much time and memory an algorithm should use. We use big O notation to express complexity, and when talking about time, the input represents the number of steps it will take to complete a task. Like if we read a single element from an array, that would give us O of one or constant time, which is extremely fast and simple. If we need to loop over an array, that would give us O of n, where n is the length of the array. But if for each iteration in that loop, we loop over the same array again, that would give us o of n squared, which would be far less efficient. However, more sophisticated algorithms like binary search cut the search area in half after each iteration, providing us with logarithmic time. Understanding how to measure complexity is essential for technical interviews and is just a good thing to think about in general. But now let's move away from discrete math and talk about statistics, which is crucial if you want to do anything with artificial intelligence, because machine learning is kind of just a fancy way of doing statistics. Like when you type into Chat GPT, it generates a response based on the probability that it fits the prompt provided by the user. At the very least, you should understand mean, median and mode, and how standard deviation indicates how close values in a set are correlated to the mean. There's a ton to learn beyond that, but what I found especially useful is the difference between linear regression and logistic regression. In a linear regression, the goal is to predict a continuous value, like the amount of money you'll lose after buying a stock. The relationship between the input and output variable is linear, and the idea is to find a line that best fits the data set. However, logistic regression is used for an entirely different set of problems, like classification, maybe an app that predicts if an image is a hot dog or not a hot dog. In this case, the relationship is not linear, but rather a sigmoid function, which might predict the probability that a Boolean variable is true or false. And that brings us to our final and most difficult topic linear algebra, which comes into play in things like computer graphics and deep neural networks. To comprehend it, you'll need to understand three weird words. Scalar is a single numeric value. A vector is a list of numbers, like a onedimensional array. And finally, a matrix is like a grid or two dimensional array that contains rows and columns of numeric values. What's cool is that vectors can represent points and directions in a 3D space, while matrices can represent transformations that happen to these vectors. When you move a player around in a video game, the lighting and shadows in the graphics change magically. But it's not magic. It's linear algebra that's being distributed on your GPU. Imagine you have a 2D vector that's at zero two x three y. It represents a single point in an image. We can perform linear transformations here, like scaling, translation, rotation, shear and so on. Let's imagine we now want to scale the point to a value of four x six y. We can represent the scaling factors in a matrix, then represent the point as a column vector. And now we can use matrix multiplication to scale the point to its target location. Operations like this are also essential in things like cryptography, such as RSA encryption, and are also essential in deep neural net works, which use vectors to represent features, then use matrix multiplication to propagate values between nodes in the network. The underlying math is surprisingly simple, but requires a massive amount of computing power to handle the staggering amount of data required. As you can see now, math is actually not that complicated, and the more you study it, the more the computer will reveal its magic tricks. Thanks for watching and I will see you in the next one.